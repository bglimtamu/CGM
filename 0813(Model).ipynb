{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c667804-3dcb-4e29-b51c-731062947e90",
   "metadata": {},
   "source": [
    "# Group 5K fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff9171a1-620b-49ab-8714-ae7de0cf976e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r0/h30ywvr55x98xwjf49klh76m0000gp/T/ipykernel_38918/3566521292.py:17: DtypeWarning: Columns (102,103) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding=\"utf-8\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÇ Fold 1\n",
      "\n",
      "üìÇ Fold 2\n",
      "\n",
      "üìÇ Fold 3\n",
      "\n",
      "üìÇ Fold 4\n",
      "\n",
      "üìÇ Fold 5\n",
      "\n",
      "üìä GroupKFold (5-fold) ÌèâÍ∑† ÏÑ±Îä•:\n",
      "\n",
      "Accuracy     0.7071\n",
      "Precision    0.1533\n",
      "Recall       0.4498\n",
      "F1           0.2277\n",
      "ROC AUC      0.6596\n",
      "PR AUC       0.1470\n",
      "dtype: float64\n",
      "\n",
      "üîç ÌèâÍ∑† Feature Importance (Top 15):\n",
      "               Feature  Importance\n",
      "57           Intensity    0.236737\n",
      "15   mean_intensity_1h    0.119838\n",
      "13     hungry_weighted    0.057979\n",
      "12            Time_24h    0.054551\n",
      "0         15m_G_Diff_5    0.040470\n",
      "50      2_2.5h_pre_std    0.036418\n",
      "44      1_1.5h_pre_std    0.034187\n",
      "47      1.5_2h_pre_std    0.027030\n",
      "1         15m_G_Diff_6    0.025320\n",
      "45    1_1.5h_pre_slope    0.024274\n",
      "2         15m_G_Diff_7    0.023953\n",
      "53        Z_Previous_4    0.021543\n",
      "29          Sleep_Diff    0.019442\n",
      "27           TimeInBed    0.017898\n",
      "62  G_minus_T_pre_1.5h    0.017619\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, average_precision_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# CSV \n",
    "file_path = '/Users/bg.lim/Downloads/TAMU_Agri/New_CGM/filtered_thre.csv'\n",
    "\n",
    "# \n",
    "df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "# Convert Time_24h to float hour\n",
    "df[\"Time_24h\"] = pd.to_datetime(df[\"Time_24h\"], format=\"%H:%M\", errors=\"coerce\")\n",
    "df[\"Time_24h\"] = df[\"Time_24h\"].dt.hour + df[\"Time_24h\"].dt.minute / 60\n",
    "\n",
    "X_columns = [\n",
    "    \"15m_G_Diff_5\", \"15m_G_Diff_6\",\"15m_G_Diff_7\",\"15m_G_Diff_8\",\n",
    "    \"15m_G_Diff_9\",\"15m_G_Diff_10\",\n",
    "    \"Z_Previous_5\",\"Z_Previous_6\",\"Z_Previous_7\",\"Z_Previous_8\",\"Z_Previous_9\",\"Z_Previous_10\",\n",
    "    \"Time_24h\", \"hungry_weighted\", \"EMA_T_Diff\",\n",
    "    \"mean_intensity_1h\", \"mean_intensity_2h\", \"mean_intensity_3h\",\n",
    "    \"HR_mean_1h\", \"HR_mean_2h\", \"HR_mean_3h\",\n",
    "    \"HR_std_1h\", \"HR_std_2h\", \"HR_std_3h\",\n",
    "    \"HR_slope_1h\", \"HR_slope_2h\", \"HR_slope_3h\",\n",
    "    \"TimeInBed\", \"Efficiency\", \"Sleep_Diff\",\n",
    "    \"TimeInBed_isnull\", \"Efficiency_isnull\", \"Sleep_Diff_isnull\",\n",
    "    \"hungry_weighted_isnull\", \"bored_weighted_isnull\", \"How_stressed_weighted_isnull\",\n",
    "    \"How_anxious_weighted_isnull\", \"How_tired_weighted_isnull\",\n",
    "    \"Glucose_range_pre_1_2.5h\",\"Glucose_spread_ratio_pre_1_2.5h\",\n",
    "    \"Glucose_skew_hint_pre_1_2.5h\",\"Glucose_std_pre_1_2.5h\",\n",
    "    \"Glucose_iqr_to_std_pre_1_2.5h\",    \n",
    "    \"1_1.5h_pre_mean\", \"1_1.5h_pre_std\", \"1_1.5h_pre_slope\",\n",
    "    \"1.5_2h_pre_mean\", \"1.5_2h_pre_std\", \"1.5_2h_pre_slope\",\n",
    "    \"2_2.5h_pre_mean\", \"2_2.5h_pre_std\", \"2_2.5h_pre_slope\",\n",
    "    \"Glucose_q25_pre_1_2.5h\",\"Z_Previous_4\",\"Z_HR\", \"Z_Intensity\",\n",
    "    \"HR\",\"Intensity\", \"1h_pre_Thre\", \"Glucose_pre_1h\", \"Glucose_pre_1.5h\",\n",
    "    \"Glucose_pre_2h\", \"G_minus_T_pre_1.5h\", \"G_minus_T_pre_2h\",\n",
    "    \"Threshold_pre_2h\", \"Threshold_pre_1.5h\"\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_model = df[X_columns + [\"GE_1h\", \"StudyID\"]].dropna()\n",
    "\n",
    "# Í∏∞Î≥∏ Îç∞Ïù¥ÌÑ∞ÏÖã Ï§ÄÎπÑ (Í∏∞Ï°¥Í≥º ÎèôÏùº)\n",
    "df_model = df[X_columns + [\"GE_1h\", \"StudyID\"]].dropna()\n",
    "X = df_model[X_columns]\n",
    "y = df_model[\"GE_1h\"].astype(int)\n",
    "groups = df_model[\"StudyID\"]\n",
    "\n",
    "# GroupKFold ÏÑ§Ï†ï\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Í≤∞Í≥º Ï†ÄÏû•Ïö© Î¶¨Ïä§Ìä∏\n",
    "metrics = []\n",
    "feature_importance_list = []\n",
    "\n",
    "threshold = 0.3  # Í≥†Ï†ï threshold\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "    print(f\"\\nüìÇ Fold {fold + 1}\")\n",
    "\n",
    "    X_train_raw, y_train_raw = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    # SMOTE Ï†ÅÏö©\n",
    "    minority_class_size = np.bincount(y_train_raw)[1]\n",
    "    k_neighbors = min(5, minority_class_size - 1) if minority_class_size > 1 else 1\n",
    "    smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "    X_train, y_train = smote.fit_resample(X_train_raw, y_train_raw)\n",
    "\n",
    "    # Î™®Îç∏ ÌïôÏäµ\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        min_samples_split=50,\n",
    "        min_samples_leaf=50,\n",
    "        max_features=None,\n",
    "        max_depth=50,\n",
    "        criterion='entropy',\n",
    "        class_weight=None,\n",
    "        bootstrap=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # ÏòàÏ∏° Î∞è ÌèâÍ∞Ä\n",
    "    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "    metrics.append({\n",
    "        \"Accuracy\": accuracy_score(y_val, y_pred),\n",
    "        \"Precision\": precision_score(y_val, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_val, y_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y_val, y_pred, zero_division=0),\n",
    "        \"ROC AUC\": roc_auc_score(y_val, y_proba),\n",
    "        \"PR AUC\": average_precision_score(y_val, y_proba)\n",
    "    })\n",
    "\n",
    "    # Feature importance Ï†ÄÏû•\n",
    "    feature_importance_list.append(model.feature_importances_)\n",
    "\n",
    "# ÌèâÍ∑† Í≤∞Í≥º Ï∂úÎ†•\n",
    "results_df = pd.DataFrame(metrics)\n",
    "print(\"\\nüìä GroupKFold (5-fold) ÌèâÍ∑† ÏÑ±Îä•:\\n\")\n",
    "print(results_df.mean().round(4))\n",
    "\n",
    "# Feature importance ÌèâÍ∑†\n",
    "feature_importance_avg = np.mean(feature_importance_list, axis=0)\n",
    "importances = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": feature_importance_avg\n",
    "}).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\nüîç ÌèâÍ∑† Feature Importance (Top 15):\")\n",
    "print(importances.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ca37013-f241-41d0-8cfb-b6656d239868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Î™®Îç∏ Ï†ÄÏû•\n",
    "joblib.dump(model, \"rf_model_last_fold.pkl\")\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•\n",
    "df_model.to_pickle(\"df_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210264ff-a446-44a9-a183-11c678492229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP events: 281\n"
     ]
    }
   ],
   "source": [
    "# y_probaÎäî ÎßàÏßÄÎßâ fold Í≤∞Í≥ºÎ•º ÏÇ¨Ïö©\n",
    "tp_mask = (y_pred == 1) & (y_val == 1)\n",
    "tp_indices = y_val.index[tp_mask]\n",
    "\n",
    "tp_enriched = df_model.loc[tp_indices].copy()\n",
    "print(f\"TP events: {len(tp_enriched)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2d5c1ada-223d-4342-8ecf-f6a0e825391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_alerts_table(alerts_df: pd.DataFrame):\n",
    "    if alerts_df is None or alerts_df.empty:\n",
    "        print(\"No alerts after policy.\")\n",
    "        empty = pd.DataFrame(columns=[\"StudyID\",\"UniqueDays\",\"TotalAlerts\",\"Alerts_per_day_mean\",\"Alerts_per_day_median\"])\n",
    "        return empty, np.nan\n",
    "\n",
    "    per_pid_day = alerts_df.groupby([\"StudyID\",\"date\"]).size().reset_index(name=\"alerts\")\n",
    "    per_pid = (per_pid_day.groupby(\"StudyID\")[\"alerts\"]\n",
    "               .agg(UniqueDays=\"count\", TotalAlerts=\"sum\",\n",
    "                    Alerts_per_day_mean=\"mean\", Alerts_per_day_median=\"median\")\n",
    "               .reset_index())\n",
    "\n",
    "    success_rate = float(alerts_df[\"success\"].mean())\n",
    "\n",
    "    print(\"\\n=== Policy-level alert summary (TABLE) ===\")\n",
    "    print(f\"Total alerts: {len(alerts_df)}\")\n",
    "    print(f\"Overall success rate: {success_rate:.3f}\")\n",
    "    print(f\"Median alerts/day across participants: {per_pid['Alerts_per_day_median'].median():.2f}\")\n",
    "    print(f\"Mean alerts/day across participants: {per_pid['Alerts_per_day_mean'].mean():.2f}\")\n",
    "\n",
    "    return per_pid, success_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0eb0e4a5-dc7c-4115-8eb2-39f29d3372b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Policy-level alert summary (TABLE) ===\n",
      "Total alerts: 632\n",
      "Overall success rate: 0.131\n",
      "Median alerts/day across participants: 1.50\n",
      "Mean alerts/day across participants: 1.71\n",
      "Event-level: TP=83, FP=549, Total=632 (SR=0.131)\n"
     ]
    }
   ],
   "source": [
    "def generate_alerts_with_policy_fast(\n",
    "    pdf: pd.DataFrame,\n",
    "    threshold: float = 0.3,\n",
    "    min_hour: float = 8.0,\n",
    "    max_hour: float = 22.0,\n",
    "    require_two_hits: bool = True,\n",
    "    two_hit_window_min: int = 30,\n",
    "    cooldown_min: int = 120,\n",
    "    per_day_cap: int = 4,\n",
    "    high_conf_single_hit_prob: float = None  # NEW: e.g., 0.60\n",
    "):\n",
    "    d = pdf.loc[(pdf[\"hour_float\"] >= min_hour) & (pdf[\"hour_float\"] <= max_hour),\n",
    "                [\"StudyID\",\"ts\",\"date\",\"y_true\",\"y_prob\"]].copy()\n",
    "    d[\"is_pos\"] = (d[\"y_prob\"] >= threshold).astype(np.uint8)\n",
    "    d = d[d[\"is_pos\"] == 1]\n",
    "    if d.empty:\n",
    "        return pd.DataFrame(columns=[\"StudyID\",\"ts\",\"date\",\"y_true_at_alert\",\"y_prob_at_alert\",\"success\"])\n",
    "\n",
    "    alerts = []\n",
    "\n",
    "    for (pid, day), g in d.sort_values([\"StudyID\",\"date\",\"ts\"]).groupby([\"StudyID\",\"date\"]):\n",
    "        times = g[\"ts\"].to_numpy()                      # numpy.datetime64[ns]\n",
    "        probs = g[\"y_prob\"].to_numpy(dtype=float)\n",
    "        yts   = g[\"y_true\"].to_numpy(dtype=int)\n",
    "\n",
    "        # ÌõÑÎ≥¥ Ïù∏Îç±Ïä§(Îëê-hit or Í≥†ÌôïÎ•† Îã®Î∞ú)\n",
    "        if require_two_hits:\n",
    "            win = np.timedelta64(two_hit_window_min, \"m\")\n",
    "            j_right = np.searchsorted(times, times + win, side=\"right\") - 1\n",
    "            has_pair = j_right > np.arange(len(times))\n",
    "            has_pair &= (times[j_right] - times) <= win\n",
    "\n",
    "            if high_conf_single_hit_prob is not None:\n",
    "                high_conf = probs >= float(high_conf_single_hit_prob)\n",
    "                cand_mask = has_pair | high_conf\n",
    "            else:\n",
    "                cand_mask = has_pair\n",
    "\n",
    "            if not np.any(cand_mask):\n",
    "                continue\n",
    "            cand_idx = np.nonzero(cand_mask)[0]\n",
    "        else:\n",
    "            cand_idx = np.arange(len(times))\n",
    "\n",
    "        selected = []\n",
    "        if len(cand_idx) > 0:\n",
    "            cooldown = np.timedelta64(cooldown_min, \"m\")\n",
    "            # TZ Í≤ΩÍ≥† ÏóÜÎäî ÏïàÏ†ÑÌïú Ï¥àÍ∏∞Ìôî: Ï≤´ Ïù¥Î≤§Ìä∏Î≥¥Îã§ Ï∂©Î∂ÑÌûà Í≥ºÍ±∞\n",
    "            last_alert_time = times[0] - cooldown - np.timedelta64(1, \"m\")\n",
    "\n",
    "            # ÌôïÎ•† ÎÜíÏùÄ ÏàúÏúºÎ°ú Í∑∏Î¶¨Îîî ÏÑ†ÌÉù\n",
    "            order = np.argsort(-probs[cand_idx])\n",
    "            for k in order:\n",
    "                i = cand_idx[k]\n",
    "                if times[i] - last_alert_time < cooldown:\n",
    "                    continue\n",
    "                selected.append(i)\n",
    "                last_alert_time = times[i]\n",
    "                if len(selected) >= per_day_cap:\n",
    "                    break\n",
    "\n",
    "        if not selected:\n",
    "            continue\n",
    "\n",
    "        sel = np.array(selected, dtype=int)\n",
    "        alerts.extend([\n",
    "            {\"StudyID\": pid, \"ts\": times[i], \"date\": day,\n",
    "             \"y_true_at_alert\": int(yts[i]), \"y_prob_at_alert\": float(probs[i])}\n",
    "            for i in sel\n",
    "        ])\n",
    "\n",
    "    if not alerts:\n",
    "        return pd.DataFrame(columns=[\"StudyID\",\"ts\",\"date\",\"y_true_at_alert\",\"y_prob_at_alert\",\"success\"])\n",
    "\n",
    "    alerts_df = pd.DataFrame(alerts)\n",
    "    alerts_df[\"success\"] = alerts_df[\"y_true_at_alert\"].astype(int)\n",
    "    return alerts_df\n",
    "\n",
    "S1 = dict(\n",
    "    threshold=0.30,\n",
    "    min_hour=8.0, max_hour=22.0,\n",
    "    require_two_hits=True,\n",
    "    two_hit_window_min=45,\n",
    "    cooldown_min=90,\n",
    "    per_day_cap=5,\n",
    "    high_conf_single_hit_prob=0.60\n",
    ")\n",
    "alerts_S1 = generate_alerts_with_policy_fast(pred_df, **S1)\n",
    "per_pid_S1, sr_S1 = summarize_alerts_table(alerts_S1)\n",
    "\n",
    "tp = int(alerts_S1[\"success\"].sum())\n",
    "total = len(alerts_S1)\n",
    "print(f\"Event-level: TP={tp}, FP={total - tp}, Total={total} (SR={tp/total:.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "583f71a9-d956-48b4-9867-84c65e20ceec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Policy-level alert summary (TABLE) ===\n",
      "Total alerts: 632\n",
      "Overall success rate: 0.131\n",
      "Median alerts/day across participants: 1.50\n",
      "Mean alerts/day across participants: 1.71\n",
      "\n",
      "[ML/S1] Event-level: TP=83, FP=549, Total=632 (SR=0.131)\n",
      "\n",
      "=== S1 policy comparison ===\n",
      "       name  total_alerts  success_rate  median_alerts_per_day  \\\n",
      "0        ML           632      0.131329                    1.5   \n",
      "1  Shuffled           791      0.083439                    2.0   \n",
      "2    Random          1006      0.102386                    2.5   \n",
      "\n",
      "   mean_alerts_per_day  \n",
      "0             1.714025  \n",
      "1             2.058470  \n",
      "2             2.515216  \n",
      "\n",
      "Lift vs Shuffled: 1.57x\n",
      "Lift vs Random:   1.28x\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 0) S1 policy (exactly as you specified) ---\n",
    "S1 = dict(\n",
    "    threshold=0.30,\n",
    "    min_hour=8.0, max_hour=22.0,\n",
    "    require_two_hits=True,\n",
    "    two_hit_window_min=45,\n",
    "    cooldown_min=90,\n",
    "    per_day_cap=5,\n",
    "    high_conf_single_hit_prob=0.60\n",
    ")\n",
    "\n",
    "# --- 1) ML alerts using S1 (no retraining) ---\n",
    "alerts_S1 = generate_alerts_with_policy_fast(pred_df, **S1)\n",
    "\n",
    "# Use the 2-return table version to avoid prior name collisions\n",
    "per_pid_S1, sr_S1 = summarize_alerts_table(alerts_S1)\n",
    "\n",
    "tp = int(alerts_S1[\"success\"].sum())\n",
    "total = len(alerts_S1)\n",
    "print(f\"\\n[ML/S1] Event-level: TP={tp}, FP={total - tp}, Total={total} (SR={tp/total:.3f})\")\n",
    "\n",
    "# If you also keep a dict-style summarize_alerts, this gives a compact summary dict:\n",
    "def summarize_alerts_dict(alerts_df: pd.DataFrame, name=\"\"):\n",
    "    if alerts_df is None or alerts_df.empty:\n",
    "        return dict(name=name, total_alerts=0, success_rate=np.nan,\n",
    "                    median_alerts_per_day=np.nan, mean_alerts_per_day=np.nan)\n",
    "    per_pid_day = alerts_df.groupby([\"StudyID\",\"date\"]).size().reset_index(name=\"alerts\")\n",
    "    per_pid = (per_pid_day.groupby(\"StudyID\")[\"alerts\"]\n",
    "               .agg([\"count\",\"sum\",\"mean\",\"median\"]).reset_index()\n",
    "               .rename(columns={\"count\":\"UniqueDays\",\"sum\":\"TotalAlerts\",\n",
    "                                \"mean\":\"Alerts_per_day_mean\",\"median\":\"Alerts_per_day_median\"}))\n",
    "    sr = float(alerts_df[\"success\"].mean())\n",
    "    return dict(\n",
    "        name=name,\n",
    "        total_alerts=int(len(alerts_df)),\n",
    "        success_rate=sr,\n",
    "        median_alerts_per_day=float(per_pid[\"Alerts_per_day_median\"].median()),\n",
    "        mean_alerts_per_day=float(per_pid[\"Alerts_per_day_mean\"].mean())\n",
    "    )\n",
    "\n",
    "sum_ml = summarize_alerts_dict(alerts_S1, name=\"ML\")\n",
    "\n",
    "# --- 2) Shuffled baseline: shuffle y_prob only, keep S1 constraints identical ---\n",
    "def run_shuffled_baseline_S1(pdf, rng=42, **policy):\n",
    "    shuffled = pdf.copy()\n",
    "    rs = np.random.RandomState(rng)\n",
    "    shuffled[\"y_prob\"] = rs.permutation(shuffled[\"y_prob\"].values)\n",
    "    al = generate_alerts_with_policy_fast(shuffled, **policy)\n",
    "    return al, summarize_alerts_dict(al, name=\"Shuffled\")\n",
    "\n",
    "alerts_shuf, sum_shuf = run_shuffled_baseline_S1(pred_df, **S1)\n",
    "\n",
    "# --- 3) Random baseline: no probabilities; same S1 constraints (except high_conf) ---\n",
    "def generate_alerts_random_same_constraints_S1(\n",
    "    pdf: pd.DataFrame,\n",
    "    min_hour: float = 8.0,\n",
    "    max_hour: float = 22.0,\n",
    "    require_two_hits: bool = True,\n",
    "    two_hit_window_min: int = 45,\n",
    "    cooldown_min: int = 90,\n",
    "    per_day_cap: int = 5,\n",
    "    seed: int = 42\n",
    "):\n",
    "    d = pdf.loc[(pdf[\"hour_float\"] >= min_hour) & (pdf[\"hour_float\"] <= max_hour),\n",
    "                [\"StudyID\",\"ts\",\"date\",\"y_true\"]].copy()\n",
    "    if d.empty:\n",
    "        return pd.DataFrame(columns=[\"StudyID\",\"ts\",\"date\",\"y_true_at_alert\",\"y_prob_at_alert\",\"success\"])\n",
    "\n",
    "    rs = np.random.RandomState(seed)\n",
    "    alerts = []\n",
    "    win = np.timedelta64(two_hit_window_min, \"m\")\n",
    "    cooldown = np.timedelta64(cooldown_min, \"m\")\n",
    "\n",
    "    for (pid, day), g in d.sort_values([\"StudyID\",\"date\",\"ts\"]).groupby([\"StudyID\",\"date\"]):\n",
    "        times = g[\"ts\"].to_numpy()\n",
    "        yts   = g[\"y_true\"].to_numpy(dtype=int)\n",
    "\n",
    "        # require two-hit window if requested\n",
    "        if require_two_hits:\n",
    "            j_right = np.searchsorted(times, times + win, side=\"right\") - 1\n",
    "            has_pair = j_right > np.arange(len(times))\n",
    "            has_pair &= (times[j_right] - times) <= win\n",
    "            cand_idx = np.nonzero(has_pair)[0]\n",
    "        else:\n",
    "            cand_idx = np.arange(len(times))\n",
    "\n",
    "        if len(cand_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        order = rs.permutation(cand_idx)  # random order\n",
    "        selected = []\n",
    "        last_alert_time = times[0] - cooldown - np.timedelta64(1, \"m\")\n",
    "        for i in order:\n",
    "            if times[i] - last_alert_time < cooldown:\n",
    "                continue\n",
    "            selected.append(i)\n",
    "            last_alert_time = times[i]\n",
    "            if len(selected) >= per_day_cap:\n",
    "                break\n",
    "\n",
    "        for i in selected:\n",
    "            alerts.append({\n",
    "                \"StudyID\": pid, \"ts\": times[i], \"date\": day,\n",
    "                \"y_true_at_alert\": int(yts[i]),\n",
    "                \"y_prob_at_alert\": np.nan,\n",
    "                \"success\": int(yts[i])\n",
    "            })\n",
    "    return pd.DataFrame(alerts)\n",
    "\n",
    "def run_random_baseline_S1(pdf, **kwargs):\n",
    "    al = generate_alerts_random_same_constraints_S1(pdf, **kwargs)\n",
    "    return al, summarize_alerts_dict(al, name=\"Random\")\n",
    "\n",
    "alerts_rand, sum_rand = run_random_baseline_S1(\n",
    "    pred_df,\n",
    "    min_hour=S1[\"min_hour\"],\n",
    "    max_hour=S1[\"max_hour\"],\n",
    "    require_two_hits=S1[\"require_two_hits\"],\n",
    "    two_hit_window_min=S1[\"two_hit_window_min\"],\n",
    "    cooldown_min=S1[\"cooldown_min\"],\n",
    "    per_day_cap=S1[\"per_day_cap\"],\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# --- 4) Compare + lifts ---\n",
    "compare = pd.DataFrame([sum_ml, sum_shuf, sum_rand])\n",
    "print(\"\\n=== S1 policy comparison ===\")\n",
    "print(compare)\n",
    "\n",
    "def lift(a, b):\n",
    "    if pd.isna(a) or pd.isna(b) or b == 0:\n",
    "        return np.nan\n",
    "    return a / b\n",
    "\n",
    "print(f\"\\nLift vs Shuffled: {lift(sum_ml['success_rate'], sum_shuf['success_rate']):.2f}x\")\n",
    "print(f\"Lift vs Random:   {lift(sum_ml['success_rate'], sum_rand['success_rate']):.2f}x\")\n",
    "\n",
    "# Optional: keep a handle for downstream analysis\n",
    "current_alerts = alerts_S1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e232c0-9d05-4e71-a12c-927ba056161a",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "94e1f11d-bb61-4767-9bda-52b3e382dac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Event-level] Best k=2, silhouette=0.234, TP n=83\n",
      "\n",
      "=== Event clusters: top +z features (top 10) ===\n",
      "\n",
      "[Cluster 0] n_events=59\n",
      "                       mean      z\n",
      "hungry_weighted       0.173  0.147\n",
      "Intensity             0.542  0.099\n",
      "mean_intensity_1h     0.424  0.047\n",
      "2_2.5h_pre_std        3.739  0.025\n",
      "Sleep_Diff          912.136  0.020\n",
      "G_minus_T_pre_1.5h  -18.318 -0.068\n",
      "TimeInBed           371.797 -0.122\n",
      "Time_24h             15.578 -0.226\n",
      "15m_G_Diff_5          1.661 -0.264\n",
      "1.5_2h_pre_std        2.181 -0.313\n",
      "\n",
      "[Cluster 1] n_events=24\n",
      "                       mean      z\n",
      "15m_G_Diff_6         14.500  1.273\n",
      "1_1.5h_pre_slope      0.967  1.273\n",
      "1_1.5h_pre_std       10.253  1.271\n",
      "15m_G_Diff_7         11.625  1.024\n",
      "Z_Previous_4          2.035  0.949\n",
      "1.5_2h_pre_std        6.187  0.769\n",
      "15m_G_Diff_5          9.042  0.650\n",
      "Time_24h             19.501  0.555\n",
      "TimeInBed           433.208  0.299\n",
      "G_minus_T_pre_1.5h  -13.904  0.168\n",
      "\n",
      "[Event-level] cluster summary (Time_24h Í∏∞Î∞ò):\n",
      "    cluster_evt  n_events  n_participants  mean_prob  median_prob  \\\n",
      "0            0        59              39      0.477        0.470   \n",
      "1            1        24              19      0.537        0.531   \n",
      "\n",
      "   mean_Time_24h  median_Time_24h  \n",
      "0         15.578           15.167  \n",
      "1         19.501           20.408  \n",
      "PCA explained variance (2D): [0.308 0.136]\n",
      "\n",
      "[Participant-level] Best k=2, silhouette=0.191, participants n=24\n",
      "\n",
      "=== Participant clusters: top +z features (top 10) ===\n",
      "\n",
      "[PID Cluster 0] n_participants=13\n",
      "                        mean      z\n",
      "Sleep_Diff          1095.308  0.197\n",
      "hungry_weighted        0.129  0.196\n",
      "Intensity              0.500  0.123\n",
      "G_minus_T_pre_1.5h   -14.706  0.041\n",
      "mean_intensity_1h      0.323 -0.012\n",
      "TimeInBed            362.115 -0.250\n",
      "1.5_2h_pre_std         2.556 -0.283\n",
      "2_2.5h_pre_std         2.611 -0.285\n",
      "15m_G_Diff_5           1.385 -0.506\n",
      "Z_Previous_4           0.318 -0.523\n",
      "\n",
      "[PID Cluster 1] n_participants=11\n",
      "                    mean      z\n",
      "Time_24h          19.238  0.833\n",
      "Time_24h          19.238  0.833\n",
      "15m_G_Diff_6      10.318  0.816\n",
      "1_1.5h_pre_slope   0.688  0.816\n",
      "1_1.5h_pre_std     7.650  0.787\n",
      "15m_G_Diff_7       6.318  0.690\n",
      "Z_Previous_4       1.668  0.618\n",
      "15m_G_Diff_5       7.818  0.598\n",
      "2_2.5h_pre_std     4.243  0.337\n",
      "1.5_2h_pre_std     4.082  0.334\n",
      "\n",
      "[Participant-level] cluster -> TP events count:\n",
      "    cluster_pid  TP_events\n",
      "0          0.0         32\n",
      "1          1.0         29\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================================\n",
    "# Inputs (Ïù¥ÎØ∏ ÏÑ∏ÏÖòÏóê ÏûàÏñ¥Ïïº Ìï®):\n",
    "# - alerts_S1 : S1 Ï†ïÏ±Ö Í≤∞Í≥º\n",
    "# - pred_df    : OoF Î©îÌÉÄÏ†ïÎ≥¥\n",
    "# - df_model   : Î™®Îç∏ ÌïôÏäµÏóê Ïì¥ ÌîºÏ≤ò ÌÖåÏù¥Î∏î\n",
    "# - X_columns  : ÌîºÏ≤ò Î¶¨Ïä§Ìä∏\n",
    "# =========================================\n",
    "\n",
    "# 0) TP ÏïåÎ¶ºÎßå Î∂ÑÏÑù\n",
    "alerts_df = alerts_S1\n",
    "assert {\"StudyID\",\"ts\",\"success\"}.issubset(alerts_df.columns), \"alerts_S1 Ïª¨Îüº ÌôïÏù∏ ÌïÑÏöî\"\n",
    "assert {\"StudyID\",\"ts\",\"idx\",\"y_prob\",\"hour_float\"}.issubset(pred_df.columns), \"pred_df Ïª¨Îüº ÌôïÏù∏ ÌïÑÏöî\"\n",
    "\n",
    "tp_alerts = alerts_df.loc[alerts_df[\"success\"] == 1].copy()\n",
    "if tp_alerts.empty:\n",
    "    raise ValueError(\"ÌòÑÏû¨ Ï†ïÏ±ÖÏóêÏÑú TP ÏïåÎ¶ºÏù¥ ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "# 1) TP ÏïåÎ¶º ‚Üî pred_df Ï°∞Ïù∏\n",
    "cols_to_merge = [\"StudyID\",\"ts\",\"idx\",\"y_prob\",\"hour_float\"]\n",
    "tp_enriched = (\n",
    "    tp_alerts.merge(pred_df[cols_to_merge],\n",
    "                    on=[\"StudyID\",\"ts\"], how=\"left\", validate=\"many_to_one\")\n",
    "    .dropna(subset=[\"idx\"])\n",
    "    .copy()\n",
    ")\n",
    "tp_enriched[\"idx\"] = tp_enriched[\"idx\"].astype(int)\n",
    "\n",
    "# 2) ÏõêÎ≥∏ ÌîºÏ≤ò ÌñâÎ†¨\n",
    "X_tp_full = df_model.loc[tp_enriched[\"idx\"].values, X_columns].reset_index(drop=True)\n",
    "\n",
    "TOP_FEATURES = [\n",
    "    \"Intensity\",\"mean_intensity_1h\",\"hungry_weighted\",\"Time_24h\",\n",
    "    \"15m_G_Diff_5\",\"2_2.5h_pre_std\",\"1_1.5h_pre_std\",\"1.5_2h_pre_std\",\n",
    "    \"15m_G_Diff_6\",\"1_1.5h_pre_slope\",\"15m_G_Diff_7\",\"Z_Previous_4\",\n",
    "    \"Sleep_Diff\",\"TimeInBed\",\"G_minus_T_pre_1.5h\"\n",
    "]\n",
    "CLUSTER_FEATURES = [f for f in TOP_FEATURES if f in X_tp_full.columns]\n",
    "if len(CLUSTER_FEATURES) == 0:\n",
    "    CLUSTER_FEATURES = list(X_tp_full.columns)[:30]\n",
    "\n",
    "X_tp = X_tp_full[CLUSTER_FEATURES].copy()\n",
    "X_tp = X_tp.replace([np.inf, -np.inf], np.nan)\n",
    "X_tp = X_tp.fillna(X_tp.median(numeric_only=True)).astype(float)\n",
    "\n",
    "# 3) Ïä§ÏºÄÏùºÎßÅ\n",
    "scaler = StandardScaler()\n",
    "X_tp_scaled = scaler.fit_transform(X_tp)\n",
    "\n",
    "# 4) k ÏÑ†ÌÉù Ìï®Ïàò\n",
    "def pick_k_and_fit(X, k_min=2, k_max=6, seed=42):\n",
    "    n = len(X)\n",
    "    if n < 2:\n",
    "        km = KMeans(n_clusters=1, random_state=seed, n_init=10).fit(X)\n",
    "        return 1, km, km.labels_, np.nan\n",
    "    k_max_eff = min(k_max, max(k_min, n))\n",
    "    best = None\n",
    "    for k in range(k_min, k_max_eff + 1):\n",
    "        km = KMeans(n_clusters=k, random_state=seed, n_init=10)\n",
    "        labels = km.fit_predict(X)\n",
    "        if len(np.unique(labels)) < 2:\n",
    "            continue\n",
    "        try:\n",
    "            s = silhouette_score(X, labels)\n",
    "        except Exception:\n",
    "            s = np.nan\n",
    "        if (best is None) or (np.isnan(best[3]) and not np.isnan(s)) or (not np.isnan(s) and s > best[3]):\n",
    "            best = (k, km, labels, s)\n",
    "    if best is None:\n",
    "        km = KMeans(n_clusters=min(2, n), random_state=seed, n_init=10).fit(X)\n",
    "        labels = km.labels_\n",
    "        try:\n",
    "            s = silhouette_score(X, labels) if len(np.unique(labels)) > 1 else np.nan\n",
    "        except Exception:\n",
    "            s = np.nan\n",
    "        return min(2, n), km, labels, s\n",
    "    return best\n",
    "\n",
    "# ----- Ïù¥Î≤§Ìä∏(ÏãúÏ†ê) ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ -----\n",
    "k_evt, km_evt, labels_evt, sil_evt = pick_k_and_fit(X_tp_scaled)\n",
    "tp_enriched[\"cluster_evt\"] = labels_evt\n",
    "sil_evt_str = \"nan\" if pd.isna(sil_evt) else f\"{sil_evt:.3f}\"\n",
    "print(f\"[Event-level] Best k={k_evt}, silhouette={sil_evt_str}, TP n={len(tp_enriched)}\")\n",
    "\n",
    "# Ïù¥Î≤§Ìä∏ ÌÅ¥Îü¨Ïä§ÌÑ∞ ÌîÑÎ°úÌååÏùº\n",
    "feat_mean = X_tp.mean(axis=0)\n",
    "feat_std  = X_tp.std(axis=0, ddof=0).replace(0, np.nan)\n",
    "\n",
    "def profile_event_cluster(c, topn=10):\n",
    "    mask = (tp_enriched[\"cluster_evt\"] == c).values\n",
    "    m = X_tp.loc[mask, :].mean(axis=0)\n",
    "    z = (m - feat_mean) / feat_std\n",
    "    return pd.DataFrame({\"mean\": m, \"z\": z}).sort_values(\"z\", ascending=False).head(topn)\n",
    "\n",
    "print(\"\\n=== Event clusters: top +z features (top 10) ===\")\n",
    "for c in range(k_evt):\n",
    "    n_ev = int((tp_enriched[\"cluster_evt\"]==c).sum())\n",
    "    print(f\"\\n[Cluster {c}] n_events={n_ev}\")\n",
    "    print(profile_event_cluster(c, topn=10).round(3))\n",
    "# Time_24h Ïª¨ÎüºÏùÑ tp_enrichedÏóê Î∂ôÏù¥Í∏∞\n",
    "tp_enriched = tp_enriched.reset_index(drop=True)\n",
    "tp_enriched[\"Time_24h\"] = X_tp[\"Time_24h\"].values\n",
    "\n",
    "\n",
    "evt_summary_time24 = (\n",
    "    tp_enriched.groupby(\"cluster_evt\")\n",
    "    .agg(n_events=(\"idx\",\"size\"),\n",
    "         n_participants=(\"StudyID\",\"nunique\"),\n",
    "         mean_prob=(\"y_prob\",\"mean\"),\n",
    "         median_prob=(\"y_prob\",\"median\"),\n",
    "         mean_Time_24h=(\"Time_24h\",\"mean\"),\n",
    "         median_Time_24h=(\"Time_24h\",\"median\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\n[Event-level] cluster summary (Time_24h Í∏∞Î∞ò):\\n\", evt_summary_time24.round(3))\n",
    "\n",
    "\n",
    "# PCA 2D Ï¢åÌëú\n",
    "try:\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X2 = pca.fit_transform(X_tp_scaled)\n",
    "    tp_enriched[\"PC1\"], tp_enriched[\"PC2\"] = X2[:,0], X2[:,1]\n",
    "    print(\"PCA explained variance (2D):\", pca.explained_variance_ratio_.round(3))\n",
    "except Exception as e:\n",
    "    print(\"PCA skipped:\", e)\n",
    "\n",
    "# =========================================\n",
    "# Ï∞∏Í∞ÄÏûê ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ\n",
    "# =========================================\n",
    "tp_feats = pd.concat([tp_enriched.reset_index(drop=True), X_tp.reset_index(drop=True)], axis=1)\n",
    "\n",
    "min_tp_per_pid = 2\n",
    "pid_counts = tp_feats.groupby(\"StudyID\")[\"idx\"].size()\n",
    "eligible_pids = pid_counts[pid_counts >= min_tp_per_pid].index\n",
    "pid_mat = tp_feats[tp_feats[\"StudyID\"].isin(eligible_pids)].groupby(\"StudyID\")[CLUSTER_FEATURES].median()\n",
    "\n",
    "pid_mat = pid_mat.replace([np.inf,-np.inf], np.nan)\n",
    "pid_mat = pid_mat.fillna(pid_mat.median(numeric_only=True)).astype(float)\n",
    "\n",
    "scaler_pid = StandardScaler()\n",
    "X_pid_scaled = scaler_pid.fit_transform(pid_mat)\n",
    "\n",
    "k_pid, km_pid, labels_pid, sil_pid = pick_k_and_fit(X_pid_scaled)\n",
    "pid_mat = pid_mat.copy()\n",
    "pid_mat[\"cluster_pid\"] = labels_pid\n",
    "sil_pid_str = \"nan\" if pd.isna(sil_pid) else f\"{sil_pid:.3f}\"\n",
    "print(f\"\\n[Participant-level] Best k={k_pid}, silhouette={sil_pid_str}, participants n={pid_mat.shape[0]}\")\n",
    "\n",
    "def profile_pid_cluster(c, topn=10):\n",
    "    Xc = pid_mat.loc[pid_mat[\"cluster_pid\"] == c, CLUSTER_FEATURES]\n",
    "    z = (Xc.mean(axis=0) - pid_mat[CLUSTER_FEATURES].mean(axis=0)) / pid_mat[CLUSTER_FEATURES].std(axis=0, ddof=0).replace(0, np.nan)\n",
    "    return pd.DataFrame({\"mean\": Xc.mean(axis=0), \"z\": z}).sort_values(\"z\", ascending=False).head(topn)\n",
    "\n",
    "print(\"\\n=== Participant clusters: top +z features (top 10) ===\")\n",
    "for c in range(k_pid):\n",
    "    n_pid = int((pid_mat['cluster_pid']==c).sum())\n",
    "    print(f\"\\n[PID Cluster {c}] n_participants={n_pid}\")\n",
    "    print(profile_pid_cluster(c, topn=10).round(3))\n",
    "\n",
    "pid_tp_counts = (\n",
    "    tp_enriched.merge(pid_mat[[\"cluster_pid\"]], left_on=\"StudyID\", right_index=True, how=\"left\")\n",
    "    .groupby(\"cluster_pid\")[\"idx\"].size().rename(\"TP_events\").reset_index()\n",
    ")\n",
    "print(\"\\n[Participant-level] cluster -> TP events count:\\n\", pid_tp_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a33c710-04a0-4676-8f1f-b9e845c54b8e",
   "metadata": {},
   "source": [
    "# Key Interpretation\n",
    "\n",
    "## Event-level\n",
    "- **Cluster 0**: Occurs around 3:30 PM; low glucose variability; slightly higher hunger and sedentary.\n",
    "- **Cluster 1**: Concentrated after 7:30 PM; very high glucose variability and spike magnitude; higher pre-event glucose levels.\n",
    "\n",
    "## Participant-level\n",
    "- **Cluster 0**: Participants with low glucose variability, slightly higher hunger and sedentary.\n",
    "- **Cluster 1**: Participants with events concentrated in the evening, showing high glucose variability and elevated pre-event glucose levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52741ef9-b4b1-4e38-ace2-d6b77ed89041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28a49b21-7e74-4d0b-bdd2-4ced9e52a421",
   "metadata": {},
   "source": [
    "# SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d02b3cf-74f1-4599-8564-21aa7fa50bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "# isnull Î≥ÄÏàò Ï†úÏô∏\n",
    "cols_no_isnull = [c for c in X_columns if not c.endswith(\"_isnull\")]\n",
    "\n",
    "# SHAP Í∞í Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ± (Ï†àÎåÄÍ∞í)\n",
    "abs_shap = np.abs(shap_values)\n",
    "shap_df = pd.DataFrame(abs_shap, columns=X_columns)\n",
    "\n",
    "# isnull Ï†úÏô∏Ìïú Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ\n",
    "shap_df_filtered = shap_df[cols_no_isnull]\n",
    "\n",
    "# Threshold Ï†ÅÏö©Ìï¥ÏÑú ÎπàÎèÑ Í≥ÑÏÇ∞\n",
    "threshold_shap = 0.01\n",
    "freq = (shap_df_filtered > threshold_shap).sum().sort_values(ascending=False)\n",
    "mean_abs = shap_df_filtered.mean().sort_values(ascending=False)\n",
    "mode_abs = shap_df_filtered.mode().iloc[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16613730-de9d-484a-a7d4-020d55fd4713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Frequency  Mean_Effect\n",
      "hungry_weighted           280     0.061343\n",
      "Intensity                 278     0.090791\n",
      "mean_intensity_1h         263     0.039572\n",
      "Time_24h                  213     0.025629\n",
      "15m_G_Diff_5              180     0.022950\n",
      "G_minus_T_pre_1.5h        165     0.016370\n",
      "2_2.5h_pre_std            145     0.020789\n",
      "15m_G_Diff_7              128     0.014066\n",
      "1_1.5h_pre_std             87     0.009001\n",
      "Z_HR                       77     0.009343\n",
      "Z_Previous_4               76     0.008751\n",
      "EMA_T_Diff                 70     0.009307\n",
      "1.5_2h_pre_std             63     0.006509\n",
      "Z_Intensity                56     0.006421\n",
      "Efficiency                 50     0.005929\n"
     ]
    }
   ],
   "source": [
    "# 1) isnull Î≥ÄÏàò Ï†úÍ±∞\n",
    "exclude_cols = [col for col in X_columns if col.endswith(\"_isnull\")]\n",
    "freq_filtered = freq.drop(index=exclude_cols, errors=\"ignore\")\n",
    "mean_abs_filtered = mean_abs.drop(index=exclude_cols, errors=\"ignore\")\n",
    "\n",
    "# 2) Îëê Í∞ú ÏãúÎ¶¨Ï¶àÎ•º ÌïòÎÇòÏùò DataFrameÏúºÎ°ú Î≥ëÌï©\n",
    "shap_summary = pd.DataFrame({\n",
    "    \"Frequency\": freq_filtered,\n",
    "    \"Mean_Effect\": mean_abs_filtered\n",
    "})\n",
    "\n",
    "# 3) Frequency Í∏∞Ï§Ä ÎÇ¥Î¶ºÏ∞®Ïàú Ï†ïÎ†¨\n",
    "shap_summary = shap_summary.sort_values(by=\"Frequency\", ascending=False)\n",
    "\n",
    "# 4) Í≤∞Í≥º ÌôïÏù∏\n",
    "print(shap_summary.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec41bd6d-e895-4817-9c89-b91164f9be96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupyter_env)",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
